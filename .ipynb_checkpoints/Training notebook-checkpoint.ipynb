{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports and globals\n",
    "import numpy as np\n",
    "from cnns.core import LasagneGoogLeNetInceptionV3 as inceptv3\n",
    "from cnns.utils import cnn_utils as cu\n",
    "from cnns.utils import process_utils as pu\n",
    "from cnns.utils import training_utils as tu\n",
    "from ds_utils import data_utils as du\n",
    "from sklearn import linear_model\n",
    "from sklearn import cross_validation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "import sys\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "cnn_dir = '/Users/babasarala/repos/cnns'\n",
    "model_dir = '%s/models'%cnn_dir\n",
    "output_dir = '/Users/babasarala/Desktop'\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect data from tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# define the categories:\n",
    "categories = {'food':0.6, 'documents':1., 'whiteboards':1., 'sketches':1., 'other':0.1} \n",
    "\n",
    "# define the model we want to use for generating CNN codes\n",
    "param_pkl_filepath = '/Users/babasarala/repos/cnns/models/inception_v3.pkl'\n",
    "model = inceptv3.LasagneGoogLeNetInceptionV3(param_pkl_filepath)\n",
    "layer = 'pool3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load in the data\n",
    "dfs = []\n",
    "cur = tu.start_psycon()\n",
    "for category in categories:\n",
    "    everalbum_sample_urls = tu.get_sample_image_urls_from_everalbum_for_category(cur, category=category, \n",
    "                                                                                 p=categories[category])\n",
    "    imagenet_sample_urls = tu.get_sample_image_urls_from_imagenet_for_category(cur, category=category, \n",
    "                                                                               p=categories[category])\n",
    "    curr_df = pd.DataFrame(everalbum_sample_urls + imagenet_sample_urls, columns=['img_url'])\n",
    "    curr_df['category'] = category\n",
    "    dfs.append(curr_df)\n",
    "\n",
    "df_img_urls = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_img_urls.to_csv('/Users/babasarala/Desktop/intermediate_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_complete_data_from_intermediate_results(results_dirpath, csv_filepath=None):\n",
    "    img_urls, X = pu.combine_checkpoint_intermediate_results(results_dirpath)\n",
    "    if csv_filepath is not None:\n",
    "        img_url_cats = pd.read_csv(csv_filepath)\n",
    "        assert 'img_url' in img_url_cats.columns\n",
    "        assert 'category' in img_url_cats.columns\n",
    "    \n",
    "    m, d = X.shape\n",
    "    feat_cols = ['f%i'%i for i in range(d)]\n",
    "    data_matrix = pd.DataFrame(data=X, columns=feat_cols)\n",
    "    data_matrix['img_url'] = img_urls\n",
    "    complete_data = pd.merge(data_matrix, img_url_cats)\n",
    "    \n",
    "    return complete_data, feat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_dirpath = '/Users/babasarala/Desktop/evaluation/dataset/productivity_food_dataset_v1.1'\n",
    "results_dirpath = '%s/GoogLeNetv3_pool3_fvs'%(dataset_dirpath)\n",
    "csv_filepath = '%s/productivity_food_dataset_v1.1.csv'%(dataset_dirpath)\n",
    "complete_data, feat_cols = construct_complete_data_from_intermediate_results(results_dirpath, csv_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other          53265\n",
      "food           14338\n",
      "documents       1373\n",
      "whiteboards      168\n",
      "sketches          69\n",
      "dtype: int64\n",
      "['food' 'sketches' 'documents' 'whiteboards' 'other']\n"
     ]
    }
   ],
   "source": [
    "print complete_data['category'].value_counts()\n",
    "categories = complete_data['category'].unique()\n",
    "print categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Train a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set seeds\n",
    "train_random_seed = np.random.randint(low=0, high=99999)\n",
    "prod_random_seed = np.random.randint(low=0, high=99999)\n",
    "\n",
    "# create mappings\n",
    "mappings = {}\n",
    "for idx, category in enumerate(categories):\n",
    "    mappings[category] = idx\n",
    "    mappings[idx] = category\n",
    "\n",
    "# tag training and testing labels\n",
    "p = 0.6\n",
    "m = complete_data.shape[0]\n",
    "train_test_cats = tu.assign_train_test(m, p=p)\n",
    "complete_data['set'] = train_test_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of sklearn.utils failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/site-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "ImportError: cannot import name warn_if_not_float\n",
      "]\n",
      "[autoreload of sklearn failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/site-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "ImportError: cannot import name __check_build\n",
      "]\n",
      "[autoreload of sklearn.svm.classes failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/site-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "ValueError: decision_function() requires a code object with 0 free vars, not 2\n",
      "]\n",
      "[autoreload of sklearn.svm.base failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/site-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "ValueError: decision_function() requires a code object with 2 free vars, not 0\n",
      "]\n",
      "[autoreload of sklearn.preprocessing.data failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/site-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "ImportError: cannot import name warn_if_not_float\n",
      "]\n",
      "[autoreload of sklearn.linear_model.base failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/site-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "ValueError: decision_function() requires a code object with 2 free vars, not 0\n",
      "]\n",
      "[autoreload of sklearn.externals.six failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/site-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "AttributeError: 'NoneType' object has no attribute 'tkinter_messagebox'\n",
      "]\n",
      "[autoreload of sklearn.feature_selection.from_model failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/site-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "ValueError: transform() requires a code object with 2 free vars, not 0\n",
      "]\n",
      "[autoreload of sklearn.externals.joblib.memory failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/site-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "ImportError: cannot import name hashing\n",
      "]\n",
      "[autoreload of sklearn.linear_model.stochastic_gradient failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/site-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "ValueError: decision_function() requires a code object with 2 free vars, not 0\n",
      "]\n",
      "[autoreload of sklearn.linear_model.coordinate_descent failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/site-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "ImportError: cannot import name cd_fast\n",
      "]\n",
      "[autoreload of sklearn.svm failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/site-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "ImportError: cannot import name libsvm\n",
      "]\n",
      "[autoreload of sklearn.metrics failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/site-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "ImportError: cannot import name cluster\n",
      "]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0b3bfdab7692>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         ])\n\u001b[1;32m      8\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier__C\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# full data, in matrix form\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, estimator, param_grid, scoring, loss_func, score_func, fit_params, n_jobs, iid, refit, cv, verbose, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    709\u001b[0m                  error_score='raise'):\n\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m         super(GridSearchCV, self).__init__(\n\u001b[0m\u001b[1;32m    712\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m             refit, cv, verbose, pre_dispatch, error_score)\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "# train a logistic regression model on this data\n",
    "train_lr = linear_model.LogisticRegression(penalty='l2', multi_class='multinomial', solver='lbfgs', \n",
    "                                           random_state=train_random_seed)\n",
    "pipe = Pipeline(steps=[\n",
    "        ('normalizer', StandardScaler()),\n",
    "        ('classifier', train_lr)\n",
    "        ])\n",
    "params = dict(classifier__C=[1e-4, 1e-3, 1e-2, 0.1, 1, 10, 1e2, 1e3, 1e4])\n",
    "train_clf = GridSearchCV(pipe, param_grid=params, cv=5)\n",
    "\n",
    "# full data, in matrix form\n",
    "X = complete_data[feat_cols].values\n",
    "y = [mappings[category] for category in complete_data['category'].values]\n",
    "\n",
    "# train-test split\n",
    "training_data =  complete_data[complete_data['set'] == 'train']\n",
    "testing_data = complete_data[complete_data['set'] == 'test']\n",
    "X_tr = training_data[feat_cols].values\n",
    "y_tr = [mappings[category] for category in training_data['category'].values]\n",
    "X_te = testing_data[feat_cols].values\n",
    "y_te = [mappings[category] for category in testing_data['category'].values]\n",
    "\n",
    "best_clf = train_clf.fit(X_tr,y_tr)\n",
    "y_pred = best_clf.predict(X_te)\n",
    "clf_report = classification_report(y_te, y_pred)\n",
    "conf_mat = confusion_matrix(y_te,y_pred)\n",
    "\n",
    "# final classifier\n",
    "best_C = train_clf.best_params_['classifier__C']\n",
    "prod_lr = linear_model.LogisticRegression(C=best_C, penalty='l2', multi_class='multinomial', solver='lbfgs', \n",
    "                                           random_state=prod_random_seed)\n",
    "prod_clf = Pipeline(steps=[\n",
    "            ('normalizer', StandardScaler()),\n",
    "            ('classifier', prod_lr)\n",
    "            ])\n",
    "prod_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.85      0.90      5663\n",
      "          1       0.92      0.50      0.65        24\n",
      "          2       0.82      0.64      0.72       547\n",
      "          3       0.97      0.84      0.90        73\n",
      "          4       0.95      0.99      0.97     21379\n",
      "\n",
      "avg / total       0.95      0.95      0.95     27686\n",
      "\n",
      "[[ 4821     0     0     0   842]\n",
      " [    0    12     5     0     7]\n",
      " [    2     0   349     2   194]\n",
      " [    0     0     7    61     5]\n",
      " [  247     1    64     0 21067]]\n"
     ]
    }
   ],
   "source": [
    "print clf_report\n",
    "print conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'food',\n",
       " 1: 'sketches',\n",
       " 2: 'documents',\n",
       " 3: 'whiteboards',\n",
       " 4: 'other',\n",
       " 'documents': 2,\n",
       " 'food': 0,\n",
       " 'other': 4,\n",
       " 'sketches': 1,\n",
       " 'whiteboards': 3}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = datetime.datetime.now()\n",
    "formatted_time = du.format_tstamp(dt, frmt='%Y-%m-%d %H_%M_%S')\n",
    "posix_time = du.datetime_to_posix(dt)\n",
    "description_str = 'L2-regularized Logistic Regression on CNN codes of Food (No drink) / Documents / \\\n",
    "                   Sketches / Whiteboards after applying Inception v3 as a filter.'\n",
    "\n",
    "data_and_model = { # metadata \n",
    "                  'unique_id': posix_time,\n",
    "                  'date_created': formatted_time,\n",
    "                  'mappings': mappings,\n",
    "                  'model_lib':model.model_lib,\n",
    "                  'model_name':model.model_name,\n",
    "                  'layer': layer,\n",
    "    \n",
    "                  # complete data\n",
    "                  'complete_data': complete_data,\n",
    "                  \n",
    "                  # training and performance eval\n",
    "                  'perc_tr': p,\n",
    "                  'training_seed': train_random_seed,\n",
    "                  'training_model':train_clf,\n",
    "                  'description': description_str,\n",
    "                  'confusion_matrix': conf_mat,\n",
    "                  'classification_report': clf_report,\n",
    "                  \n",
    "                  # production classifier\n",
    "                  'production_seed': prod_random_seed,\n",
    "                  'prod_model': prod_clf\n",
    "                  }\n",
    "\n",
    "model_only = {'unique_id':posix_time, \n",
    "              'mappings':mappings, \n",
    "              'date_created': formatted_time, \n",
    "              'prod_model': prod_clf}\n",
    "\n",
    "data_and_model_filename_str = '%s_L1_LR.p'%(formatted_time)\n",
    "data_and_model_savepath = '%s/%s'%(model_dir, data_and_model_filename_str)\n",
    "model_only_filename_str = '%s_model_only_L1_LR.p'%(formatted_time)\n",
    "model_only_savepath = '%s/%s'%(model_dir, model_only_filename_str)\n",
    "\n",
    "if os.path.isfile(data_and_model_savepath):\n",
    "    print 'This file already exists!!'\n",
    "else:\n",
    "    pkl.dump(data_and_model, open(data_and_model_savepath, 'wb'))\n",
    "\n",
    "if os.path.isfile(model_only_savepath):\n",
    "    print 'This file already exists!!'\n",
    "else:\n",
    "    pkl.dump(model_only, open(model_only_savepath, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
