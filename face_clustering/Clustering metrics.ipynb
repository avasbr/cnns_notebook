{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cnns.utils import clustering_utils as clu\n",
    "from cnns.utils import cnn_utils as cu\n",
    "from cnns.core import SOMClusterer as scl\n",
    "from cnns.core import APClusterer as ap\n",
    "from sklearn.metrics.cluster import homogeneity_completeness_v_measure\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# computes h, c, v given a single set of parameters for multi-som\n",
    "def compute_hcv_metrics(user_id, vis_tag='benchmark', grid_size=15, sigma=1.0, \n",
    "                        learning_rate=0.25, num_trials=5, num_iter=100, \n",
    "                        reduce_dim=False, num_dim=512):\n",
    "    \n",
    "    # keep these hard-coded for the time being...\n",
    "    base_dirpath = '/Users/babasarala/Desktop/face_clustering_full_data'\n",
    "    cluster_settings_ver = 1.2\n",
    "    comp_type = 'None'\n",
    "    tagged_csv_filepath = '%s/%i_clusters.csv'%(base_dirpath, user_id)\n",
    "    tagged_df = pd.read_csv(open(tagged_csv_filepath, 'rb'))\n",
    "    \n",
    "    # combine with the CSV file\n",
    "    url_csv_filepath = '%s/%i/%i_prod_fb_tags_img_urls_%.1f_%s.csv'\\\n",
    "                       %(base_dirpath, user_id, user_id, cluster_settings_ver,comp_type)\n",
    "    url_df = pd.read_csv(open(url_csv_filepath, 'rb'))\n",
    "    \n",
    "    pkl_filepath = '%s/%i/%i_prod_1.2_None_cnn_codes.p'%(base_dirpath, user_id, user_id)\n",
    "    img_urls, X = pkl.load(open(pkl_filepath, 'rb'))\n",
    "    scl_ = scl.SOMClusterer(img_urls, X, grid_size=grid_size, sigma=sigma, learning_rate=learning_rate, \n",
    "                            num_trials=num_trials, reduce_dim=reduce_dim, num_dim=num_dim, num_iter=num_iter)\n",
    "    cluster_df = scl_.run()\n",
    "    \n",
    "    gt_df = pd.merge(cluster_df, url_df, left_on='face_url', right_on='img_url')\n",
    "    cols = ['face_id', 'tag', 'memorable_id', 'cluster_idx', 'face_url']\n",
    "\n",
    "    merged_df = pd.merge(gt_df, tagged_df, on=['face_id', 'memorable_id', 'user_id'])[cols]\n",
    "    true_labels = list(merged_df['tag'].values)\n",
    "    cluster_labels = list(merged_df['cluster_idx'].values)\n",
    "    h, c, v = homogeneity_completeness_v_measure(true_labels, cluster_labels)\n",
    "    \n",
    "    # visualize!\n",
    "    #cluster_map, _, _ = scl_.convert_to_maps(cluster_df)\n",
    "    #clu.visualize_clusters(cluster_map, '%s/%i/cluster_htmls/%i_%s_visualization.html'%(base_dirpath, \n",
    "    #                                                                                    user_id,\n",
    "    #                                                                                    user_id,\n",
    "    #                                                                                    vis_tag))\n",
    "    return h, c, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.94467167761099868, 0.62587234451776363, 0.75291602060862328)\n",
      "(0.91164197472455044, 0.51806201456998691, 0.66067812852002117)\n",
      "(0.95525492998131911, 0.4696769464768778, 0.62973006083041616)\n",
      "(0.94488535690170561, 0.57847139005902215, 0.71761148128152297)\n",
      "(0.96386913518529493, 0.49705294897486607, 0.65587891546607846)\n",
      "(0.91953177353443116, 0.3297790670232067, 0.48545537352261375)\n",
      "(0.93353778957693501, 0.48513913269593822, 0.63847618363827308)\n",
      "(0.950062362614707, 0.62928898957117096, 0.75710041767709746)\n",
      "(0.9679689081541154, 0.540678759068148, 0.69381372397033303)\n",
      "(0.84222229416584626, 0.36225281586072056, 0.50660639659963491)\n",
      "(0.95407190857232782, 0.43568679836638086, 0.59819957692197023)\n",
      "(0.94392129818968118, 0.55497531289178093, 0.69898485850878211)\n",
      "(0.95111001747459334, 0.66656405547588982, 0.78381147482360047)\n",
      "(0.90468830983014625, 0.67594288002357661, 0.77376383003911597)\n"
     ]
    }
   ],
   "source": [
    "user_ids = [1946418, 8657185, 5626377, 5, 5692777, 3473194, 3928074, 4619758, 2685009, 1496616, 1341, 8, 34, 6007945]\n",
    "for user_id in user_ids:\n",
    "    print compute_hcv_metrics(user_id, vis_tag='hyperopt',grid_size=8, sigma=2.39, \n",
    "                        learning_rate=0.0002, num_trials=13, num_iter=366, \n",
    "                        reduce_dim=False, num_dim=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# computes h, c, v given a single set of parameters for affinity propagation\n",
    "def compute_hcv_metrics_ap(user_id, damping=0.5, convergence_iter=15, max_iter=200, affinity='euclidean', \n",
    "                           reduce_dim=False, num_dim=512):\n",
    "    \n",
    "    # keep these hard-coded for the time being...\n",
    "    base_dirpath = '/Users/babasarala/Desktop/face_clustering_full_data'\n",
    "    cluster_settings_ver = 1.2\n",
    "    comp_type = 'None'\n",
    "    tagged_csv_filepath = '%s/%i_clusters.csv'%(base_dirpath, user_id)\n",
    "    tagged_df = pd.read_csv(open(tagged_csv_filepath, 'rb'))\n",
    "    \n",
    "    # combine with the CSV file\n",
    "    url_csv_filepath = '%s/%i/%i_prod_fb_tags_img_urls_%.1f_%s.csv'\\\n",
    "                       %(base_dirpath, user_id, user_id, cluster_settings_ver,comp_type)\n",
    "    url_df = pd.read_csv(open(url_csv_filepath, 'rb'))\n",
    "    \n",
    "    pkl_filepath = '%s/%i/%i_prod_1.2_None_cnn_codes.p'%(base_dirpath, user_id, user_id)\n",
    "    img_urls, X = pkl.load(open(pkl_filepath, 'rb'))\n",
    "    ap_ = ap.APClusterer(img_urls, X, damping=damping, convergence_iter=convergence_iter, \n",
    "                         max_iter=max_iter, affinity=affinity)\n",
    "    cluster_df = ap_.run()\n",
    "    \n",
    "    gt_df = pd.merge(cluster_df, url_df, left_on='face_url', right_on='img_url')\n",
    "    cols = ['face_id', 'tag', 'memorable_id', 'cluster_idx', 'face_url']\n",
    "\n",
    "    merged_df = pd.merge(gt_df, tagged_df, on=['face_id', 'memorable_id', 'user_id'])[cols]\n",
    "    true_labels = list(merged_df['tag'].values)\n",
    "    cluster_labels = list(merged_df['cluster_idx'].values)\n",
    "    h, c, v = homogeneity_completeness_v_measure(true_labels, cluster_labels)\n",
    "    \n",
    "    # visualize!\n",
    "    cluster_map, _, _ = ap_.convert_to_maps(cluster_df)\n",
    "    clu.visualize_clusters(cluster_map, '%s/%i/cluster_htmls/%i_ap_benchmark_visualization.html'%(base_dirpath, \n",
    "                                                                                                  user_id, \n",
    "                                                                                                  user_id))\n",
    "    return h, c, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.81691319967933962, 0.64167025864308913, 0.71876436159516643)\n",
      "(0.86055756673398431, 0.5170361700768773, 0.64596604433590632)\n",
      "(0.87644561697223256, 0.47876248035198909, 0.61925438359429696)\n",
      "(0.89696463651039604, 0.58480595628621734, 0.70800468650059223)\n",
      "(0.64932365211847387, 0.55611640291205233, 0.59911653380839103)\n",
      "(0.915168703284297, 0.34337817485148342, 0.49938379646285597)\n",
      "(0.93365237009987734, 0.48579180580132514, 0.63906799374275036)\n",
      "(0.83236176952702645, 0.99999999999999989, 0.90851248194495748)\n",
      "(0.83387808223614646, 0.549254278999731, 0.66228094673895699)\n",
      "(0.92002315855883798, 0.35599702298575048, 0.51335474197348907)\n",
      "(0.9417033073552824, 0.32330699834520549, 0.48135460756455622)\n",
      "(0.91600018566019303, 0.52515532224526262, 0.66757871727005458)\n",
      "(0.97204690973654628, 0.71366727676827602, 0.82305538686965873)\n",
      "(0.91801120244806245, 0.64788726221480875, 0.75965048572245686)\n"
     ]
    }
   ],
   "source": [
    "user_ids = [1946418, 8657185, 5626377, 5, 5692777, 3473194, 3928074, 4619758, 2685009, 1496616, 1341, 8, 34, 6007945]\n",
    "for user_id in user_ids:\n",
    "    print compute_hcv_metrics_ap(user_id, reduce_dim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "110\n",
      "66\n",
      "134\n",
      "33\n",
      "50\n",
      "97\n",
      "13\n",
      "58\n",
      "85\n",
      "108\n",
      "120\n",
      "12\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "for user_id in user_ids:\n",
    "    base_dirpath = '/Users/babasarala/Desktop/face_clustering_full_data'\n",
    "    tagged_csv_filepath = '%s/%i_clusters.csv'%(base_dirpath, user_id)\n",
    "    df = pd.read_csv(open(tagged_csv_filepath, 'rb'))\n",
    "    print len(df['tag'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance for repeated trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_id = 5626377\n",
    "H = np.empty((2, 10))\n",
    "C = np.empty((2, 10))\n",
    "V = np.empty((2, 10))\n",
    "\n",
    "for idx, num_iter in enumerate([500]):\n",
    "    for i in range(10):\n",
    "        h, c, v = compute_hcv_metrics(user_id, num_iter=num_iter)\n",
    "        H[idx, i] = h\n",
    "        C[idx, i] = c\n",
    "        V[idx, i] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.95675248,  0.97949008,  0.9713768 ,  0.97129576,  0.96846991,\n",
       "        0.96413817,  0.97463498,  0.96487064,  0.9724939 ,  0.9733207 ])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity mean, std\n",
      "[ 0.96968434  0.        ]\n",
      "[ 0.00608177  0.        ]\n",
      "Completeness mean, std\n",
      "[ 0.42857745  0.        ]\n",
      "[ 0.00670511  0.        ]\n",
      "V-Measure mean, std\n",
      "[  5.94392086e-001   4.94065646e-324]\n",
      "[ 0.00649097  0.        ]\n"
     ]
    }
   ],
   "source": [
    "print 'Homogeneity mean, std'\n",
    "print np.mean(H, axis=1)\n",
    "print np.std(H, axis=1)\n",
    "\n",
    "print 'Completeness mean, std'\n",
    "print np.mean(C, axis=1)\n",
    "print np.std(C, axis=1)\n",
    "\n",
    "print 'V-Measure mean, std'\n",
    "print np.mean(V, axis=1)\n",
    "print np.std(V, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing with number of iterations: 10\n",
      "Currently processing with number of iterations: 50\n",
      "Currently processing with number of iterations: 100\n",
      "Currently processing with number of iterations: 500\n",
      "Currently processing with number of iterations: 1000\n"
     ]
    }
   ],
   "source": [
    "user_id = 5626377\n",
    "num_iters = [10, 50, 100, 500, 1000]\n",
    "hs = []\n",
    "cs = []\n",
    "vs = []\n",
    "for num_iter in num_iters:\n",
    "    print 'Currently processing with number of iterations: %i'%(num_iter)\n",
    "    h, c, v = compute_hcv_metrics(user_id, num_iter=num_iter)\n",
    "    hs.append(h)\n",
    "    cs.append(c)\n",
    "    vs.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.42340902098957089,\n",
       " 0.4326177007735828,\n",
       " 0.41429745914816557,\n",
       " 0.43944550881313643,\n",
       " 0.43179167359057719]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing with number of dimensions: 128\n",
      "(2520, 128)\n",
      "-3.32997156363e-16 1.0 -6.65994312725e-16\n",
      "Currently processing with number of dimensions: 256\n",
      "(2520, 256)\n",
      "0.0779097786969 0.199353388764 0.11203492006\n",
      "Currently processing with number of dimensions: 512\n",
      "(2520, 512)\n",
      "0.673383748645 0.856998259316 0.754176012837\n",
      "Currently processing with number of dimensions: 1024\n",
      "(2520, 1024)\n",
      "0.768963574699 0.740566930956 0.754498160055\n",
      "Currently processing with number of dimensions: 2048\n",
      "(2520, 2048)\n",
      "0.7901482358 0.486010644583 0.601837998858\n",
      "0.850587368382 0.422252389098 0.564348412791\n"
     ]
    }
   ],
   "source": [
    "num_dims = [128, 256, 512, 1024, 2048]\n",
    "user_id = 1496616\n",
    "for num_dim in num_dims:\n",
    "    print 'Currently processing with number of dimensions: %i'%(num_dim)\n",
    "    h, c, v = compute_hcv_metrics(user_id, reduce_dim=True, num_dim=num_dim)\n",
    "    print h, c, v\n",
    "h, c, v = compute_hcv_metrics(user_id)\n",
    "print 'Currently processing with number of dimensions: 4096'\n",
    "print h, c, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute H-C-V metrics\n",
    "def compute_hcv_metrics(tagged_csv_filepath, cluster_df, , vis_tag='benchmark', grid_size=15, sigma=1.0, \n",
    "                        learning_rate=0.25, num_trials=5, num_iter=100, \n",
    "                        reduce_dim=False, num_dim=512):\n",
    "    \n",
    "    # keep these hard-coded for the time being...\n",
    "    base_dirpath = '/Users/babasarala/Desktop/face_clustering_full_data'\n",
    "    cluster_settings_ver = 1.2\n",
    "    comp_type = 'None'\n",
    "    tagged_csv_filepath = '%s/%i_clusters.csv'%(base_dirpath, user_id)\n",
    "    tagged_df = pd.read_csv(open(tagged_csv_filepath, 'rb'))\n",
    "    \n",
    "    # combine with the CSV file - we need this\n",
    "    url_csv_filepath = '%s/%i/%i_prod_fb_tags_img_urls_%.1f_%s.csv'\\\n",
    "                       %(base_dirpath, user_id, user_id, cluster_settings_ver,comp_type)\n",
    "    url_df = pd.read_csv(open(url_csv_filepath, 'rb'))\n",
    "    \n",
    "    pkl_filepath = '%s/%i/%i_prod_1.2_None_cnn_codes.p'%(base_dirpath, user_id, user_id)\n",
    "    img_urls, X = pkl.load(open(pkl_filepath, 'rb'))\n",
    "    scl_ = scl.SOMClusterer(img_urls, X, grid_size=grid_size, sigma=sigma, learning_rate=learning_rate, \n",
    "                            num_trials=num_trials, reduce_dim=reduce_dim, num_dim=num_dim, num_iter=num_iter)\n",
    "    cluster_df = scl_.run()\n",
    "    \n",
    "    gt_df = pd.merge(cluster_df, url_df, left_on='face_url', right_on='img_url')\n",
    "    cols = ['face_id', 'tag', 'memorable_id', 'cluster_idx', 'face_url']\n",
    "\n",
    "    merged_df = pd.merge(gt_df, tagged_df, on=['face_id', 'memorable_id', 'user_id'])[cols]\n",
    "    true_labels = list(merged_df['tag'].values)\n",
    "    cluster_labels = list(merged_df['cluster_idx'].values)\n",
    "    h, c, v = homogeneity_completeness_v_measure(true_labels, cluster_labels)\n",
    "    \n",
    "    # visualize!\n",
    "    #cluster_map, _, _ = scl_.convert_to_maps(cluster_df)\n",
    "    #clu.visualize_clusters(cluster_map, '%s/%i/cluster_htmls/%i_%s_visualization.html'%(base_dirpath, \n",
    "    #                                                                                    user_id,\n",
    "    #                                                                                    user_id,\n",
    "    #                                                                                    vis_tag))\n",
    "    return h, c, v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
